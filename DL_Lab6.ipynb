{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhask94/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "## Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "## There are two parts of this lab:\n",
        "###  1.   Wiring up a basic sequence-to-sequence computation graph\n",
        "###  2.   Implementing your own GRU cell.\n",
        "\n",
        "\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7bdZWxvJrsx",
        "colab": {}
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "from IPython.core.debugger import set_trace\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxBeKeNjJ0NQ",
        "outputId": "ec5dd044-7b8a-4317-fdb3-dc547e0fced1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "istener; for \n",
            "Frodo showed no sign of weariness and made no attempt to change the subject, \n",
            "though actually he soon got rather lost among the strange names of people \n",
            "and places that he had never heard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On0_WitWJ99e",
        "outputId": "a3bf357e-f4ee-4f99-d296-d9dff4546455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# class TextDataset(Dataset):\n",
        "#   def __init__(self, chunk_len=200, filename='data.txt'):\n",
        "#     root = 'data/'\n",
        "    \n",
        "#     text_files = os.listdir(root)\n",
        "#     self.training_file = text_files[text_files.index(filename)]\n",
        "#     with open(os.path.join(root, self.training_file), encoding='utf-8') as file:\n",
        "#       self.training_file = file.read()\n",
        "#     self.segment_extractor = FileSegmentExtractor(self.training_file, chunk_len)\n",
        "    \n",
        "#   def extract_zip(self, zip_path):\n",
        "#     print('Unzipping {}'.format(zip_path))\n",
        "#     with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "#       zip_ref.extractall(os.path.dirname(self.root))\n",
        "  \n",
        "#   def __len__(self):\n",
        "#     return self.len\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "#   return Variable(tensor)\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavAv50ZKQ-F",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = num_layers\n",
        "    \n",
        "    self.w_ir = []\n",
        "    self.w_hr = []\n",
        "    self.w_iz = []\n",
        "    self.w_hz = []\n",
        "    self.w_in = []\n",
        "    self.w_hn = []\n",
        "    \n",
        "    for l in range(self.n_layers):\n",
        "      self.w_ir.append(nn.Linear(input_size,  hidden_size))\n",
        "      self.w_hr.append(nn.Linear(hidden_size, hidden_size))\n",
        "      self.w_iz.append(nn.Linear(input_size,  hidden_size))\n",
        "      self.w_hz.append(nn.Linear(hidden_size, hidden_size))\n",
        "      self.w_in.append(nn.Linear(input_size,  hidden_size))\n",
        "      self.w_hn.append(nn.Linear(hidden_size, hidden_size))\n",
        "    \n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.tan = nn.Tanh()\n",
        "     \n",
        "  def forward(self, inputs, prev_hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    \n",
        "    hidden = torch.zeros(prev_hidden.shape)\n",
        "    for l in range(self.n_layers):\n",
        "      r_t = self.sig(self.w_ir[l](inputs) + self.w_hr[l](prev_hidden[l]))\n",
        "      z_t = self.sig(self.w_iz[l](inputs) + self.w_hz[l](prev_hidden[l]))\n",
        "      n_t = self.tan(self.w_in[l](inputs) + self.w_hn[l](prev_hidden[l]))\n",
        "      hidden[l] = (1 - z_t) * n_t + z_t * prev_hidden[l]\n",
        "    output = hidden[l:l+1]\n",
        "    \n",
        "    return output, hidden\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6tNdEnzWj5F",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.gru = GRU(input_size=hidden_size, hidden_size=hidden_size, \n",
        "                      num_layers=n_layers)\n",
        "    self.decode = nn.Linear(self.hidden_size, self.output_size)\n",
        "#     self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    embed = self.embedding(input_char).view(1,1,-1)\n",
        "    output, hidden = self.gru(embed, hidden)\n",
        "#     set_trace()\n",
        "    out_decoded = self.relu(self.decode(output))\n",
        "    \n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "#     return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
        "    return torch.randn(self.n_layers, 1, self.hidden_size) #changed from zeros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhXghEPKD-5",
        "colab": {}
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ALC3Pf8Kbsi",
        "colab": {}
      },
      "source": [
        "def train(input_str, target_str):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "  \n",
        "  for in_char, target_char in zip(input_str, target_str):\n",
        "    \n",
        "    char_hat, new_hidden = decoder(in_char, hidden)\n",
        "#     set_trace()\n",
        "    target_char = target_char.unsqueeze(0)\n",
        "    loss += criterion(char_hat.squeeze(0), target_char)\n",
        "    \n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  \n",
        "  return loss.item() #, len(input_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-bp-OZ1KjNh",
        "colab": {}
      },
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden variable, initialize other useful variables \n",
        "    # your code here\n",
        "  ## /\n",
        "  hidden = decoder.init_hidden()\n",
        "  prediction = prime_str + '' # copies prime_str values, not a ptr\n",
        "  primer_input = char_tensor(prime_str)\n",
        "  all_chars = string.printable \n",
        "\n",
        "  for char in primer_input[:-1]:\n",
        "    _, hidden = decoder(char, hidden)\n",
        "  \n",
        "  in_char = primer_input[-1]\n",
        "  \n",
        "  for p in range(predict_len):\n",
        "    out_char, hidden = decoder(in_char, hidden)\n",
        "    out_dist = out_char.data.view(-1).div(temperature).exp()\n",
        "    top_i = torch.multinomial(out_dist, 1)[0]\n",
        "    \n",
        "    char_decoded = all_chars[top_i]\n",
        "    in_char = char_tensor(char_decoded)\n",
        "    prediction += char_decoded\n",
        "    \n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nXFeCmdKodw",
        "colab": {}
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "daf21373-f90b-48cf-cdda-e8831ebf1735"
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[129.53036069869995 (200 4%) 833.7415]\n",
            "WhdlcdOz7\rC9av{&,v *f+Gb0-9e5sE.lURv`;\ringdZ!U3NkLIJnf*J!b,A8U7\t<\"oFi\fP\"9of-'/QmmZp hZ([d:eIsZN,ptpK2I \n",
            "\n",
            "[236.2918839454651 (400 8%) 725.3318]\n",
            "Wheede the arellll>(ruW\u000b \n",
            "\n",
            "ng?;d weye wmer ouiar\n",
            "a id`yM[z _y+neW isthaind the and the frtheirerer het \n",
            "\n",
            "[348.01480746269226 (600 12%) 569.2349]\n",
            "Fqquu\u000byis th t hos the ad  wasast hir timhear oupith to f atwe th et on \n",
            "\n",
            "[461.5968496799469 (800 16%) 583.6088]\n",
            "Wh tahife hes t oator larone t athe t n o nd s theo roond s ang aring thee thehe t angsind ir hee mand \n",
            "\n",
            "[572.1966166496277 (1000 20%) 550.7297]\n",
            "Whee s t hithians \n",
            "n ltis arsste halind o t hin uthe taton hathore nat t higr nd o th ator thid e that \n",
            "\n",
            "[681.9578876495361 (1200 24%) 579.3874]\n",
            "Whee ast ad th hit the it as whe avor ithe re nd thaind obere hat ly outh ha atir n an t sthi st the a \n",
            "\n",
            "[791.8179275989532 (1400 28%) 532.3194]\n",
            "Whe t ass coouro theing od the atin, bo f t at f the toor ast ato wha st hats ath ot he athe the ts th \n",
            "\n",
            "[901.4364812374115 (1600 32%) 521.4766]\n",
            "Wher ind asas ont s cand s bou fo t se y wind the as pe s tah the an orn orr lid on tollind ild is ori \n",
            "\n",
            "[1009.6915390491486 (1800 36%) 518.1337]\n",
            "Wheay d nid  tto aro st he mat thae ast ild d tho dr ard  n llit has as r ste pt wtoron s athe toaroun \n",
            "\n",
            "[1121.517462015152 (2000 40%) 481.2728]\n",
            "Whise t  ast thi at the ang it or fil, \n",
            "fpor asit t ho at oway e or  chaind gount he ind se n alle the \n",
            "\n",
            "[1237.9550631046295 (2200 44%) 510.7132]\n",
            "Whea, carco the siry the thed d bat ato han thee thilas there aware s the l alll thee as ar malve e he \n",
            "\n",
            "[1339.990669965744 (2400 48%) 506.3060]\n",
            "Whe y thomeo con toroseta ld w itros e my ha ast heassst hine sthe thee athe athe  thoun thinge t owed \n",
            "\n",
            "[1441.0952796936035 (2600 52%) 482.1031]\n",
            "Whee  thaes bee mavond thalre s th lest ho bathe wamd at anth thowin that uit d aor stor tok go thhe t \n",
            "\n",
            "[1545.6186938285828 (2800 56%) 553.6956]\n",
            "Whee id the y and thee hats thee  thas thee an thar s tlthee rten carrint st the a be \n",
            "l irthe s fa sl \n",
            "\n",
            "[1653.8867816925049 (3000 60%) 538.3664]\n",
            "Whe r ackaitilort own the wint ad  lor t hee rte s t hiit siy suthes. the le the ant thee whe ise t sa \n",
            "\n",
            "[1756.6461629867554 (3200 64%) 496.3288]\n",
            "Why the tout as st thoar d t her ta thoreay t s fo t the theins ithoarng it t t ars st the t ast s tr  \n",
            "\n",
            "[1857.328242301941 (3400 68%) 499.5669]\n",
            "Whee talllacre nd thar sised the ad ill at at t iwande rtoree ass toire t ome otheam ass fan the ysan  \n",
            "\n",
            "[1958.5430226325989 (3600 72%) 531.7987]\n",
            "Whe e str he ot or. thied a to lee s thirend oon ghe andor th sst heowt mit on and s thte os owie itho \n",
            "\n",
            "[2059.452701330185 (3800 76%) 529.2330]\n",
            "Whe as bind t thin td he s t and ond ack adt t waind thern. ond ile indo he trdit th out ithal \n",
            "the as \n",
            "\n",
            "[2160.08327960968 (4000 80%) 506.1932]\n",
            "Whormin ' hithatof  \n",
            "\n",
            "\n",
            "Sis in wan d ble acon thaite as hed t the s, and ang the o wathe ss of lame the \n",
            "\n",
            "[2261.4656314849854 (4200 84%) 549.7640]\n",
            "Whin as thithoat ilie d the t ht heor for thirne tche halbe athe the ampn thansdo theamy a go thre f h \n",
            "\n",
            "[2362.611635684967 (4400 88%) 482.2274]\n",
            "Whapeawe \n",
            "\n",
            "g \n",
            "t hatyriowrey che cast thint thees ithe ime s thant f thiit was ste, irst ild in t he le \n",
            "\n",
            "[2463.950938463211 (4600 92%) 498.8387]\n",
            "Whe f at ist ant haid sere the ad ist hindo s tho st he ste s he the wat ha the atr the theat tthe ath \n",
            "\n",
            "[2564.996433734894 (4800 96%) 486.2952]\n",
            "Whe at he t \n",
            "\n",
            "\n",
            "anrn 'Yound esuthe tho sim and s wand oonk t ind and  th akind ther the thee hind dond  \n",
            "\n",
            "[2665.7401883602142 (5000 100%) 494.8545]\n",
            "Whan ang f and sane and hean tharintolo ans the ay f wo athe the ee d was want asthe, oothe f ohre the \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "95188cdd-ec27-43af-f623-97035424358d"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ca\n",
            " caldo aty air a was han ong he thaind  the ande s was hornrs ouse rit. ' thes the oo the atheot the ork at hat haim tamy ag ou t hase ast hein bisas ofant he ar and tha whe ain ind ande \n",
            "\n",
            "as found olo c \n",
            "\n",
            " ca\n",
            " casad oor o she d hens theag tamint ournd le ther s ald.  the ameny them utin y s thount the sth ond e than we ing arhee st of he the s thine ow she irt the to t heang dan ton hte wowan than loe fo and  \n",
            "\n",
            " ca\n",
            " cato sanothe thee y lloulke d any an  hinghitr co ange ronoucke ally od in and o thino athhears to ornd o Burr re  ald s thethe g ats ot us he wa mas ise athas t arle sthang as wthen calolt wos ss co of \n",
            "\n",
            " lo\n",
            " lo than wang k lan itto wathear by atl helsst tha ngas t and theathe as alllored ad e halot s the athor te \n",
            "the tlole d ano wid wher as tan. he aind d tthe the d as they th ave re ad sailke athe meeve a \n",
            "\n",
            " Th\n",
            " Thit hain anthe cathaind oond o wo thin tors thr thean, was thee ang the of ound the am bpand t ancore coront hit the t heom t ome d an nde ht wind s d s on win and isn the \n",
            "\n",
            "\n",
            "\n",
            "we the art, t hane topof  \n",
            "\n",
            " wh\n",
            " whe am t hes he t ho the wo h hits wagt hot athido the athe o waind o lthand of lthe to wier er tha and oan they e othe wind an ging hanome d n of dom at t hase and an wit tharf sthe nd asu o arflot ous \n",
            "\n",
            " I \n",
            " I f pid twis s oan r a urno ar the let t tharond land  ofor athen thinge s int ar se hand an bares t wale d the alllile walowo that hon core wand it hill sain the the ay thom thaike t bund otito ang and \n",
            "\n",
            " Th\n",
            " Tho y ithe hamoinm or y of the thoon l \n",
            "t wu ind l of p ar me ant thaure sn e the ithe  wate on bend ancoses, awine t hade ad ras he tho Hilomests e as nth hid s anout hancoo arearse as cangad o the d s \n",
            "\n",
            " ca\n",
            " cakis anle s ban tat he aof y \n",
            "\n",
            "int theer thard siasnd daint s ofoo caarirur \n",
            "sathe thor m re stho fr ind \n",
            "sthe ind s the mine he athe aso whing of ongofr as wok thas al the an rer oout the the as thes  \n",
            "\n",
            " I \n",
            " I nto hea woword. is the anghere hit athe  fthe theaindo abour hat ang pe tho thorut athe ats caluto llnd oncoullind ' and mis id ang s nae thare arid or hed and f the are  the seon was ais sons anl war \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    }
  ]
}