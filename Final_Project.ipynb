{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Final_Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhask94/cs474_labs_f2019/blob/conv2linear/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkpLgBkLMSCk",
        "colab_type": "code",
        "outputId": "a4f77b93-406d-4789-faa8-1074cb792e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# imports for training data\n",
        "!if ( ! ls . | grep pytransform ); then git clone https://github.com/mhask94/pytransform.git; fi\n",
        "# !git clone https://github.com/mhask94/pytransform.git\n",
        "from pytransform.common import skew\n",
        "from pytransform.quaternion import Quaternion as Quat\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytransform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFbSU4rzH849",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports for pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73-GfR1p7usv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classes for quadrotor state and dynamics\n",
        "class State():\n",
        "    def __init__(self, arr=np.empty(0)):\n",
        "        if len(arr) == 0:\n",
        "            self.arr = np.zeros((10,1), dtype=np.float64)\n",
        "            self.arr[3] = 1\n",
        "        else:\n",
        "            assert arr.shape == (10, 1)\n",
        "            if not arr.dtype == np.float64:\n",
        "              arr = np.array(arr, dtype=np.float64)\n",
        "            arr.dtype = np.float64\n",
        "            self.arr = arr\n",
        "\n",
        "    def __getitem__(self, position):\n",
        "        return self.arr[position]\n",
        "    def __str__(self):\n",
        "        s = 'p: ' + str(self.p.flatten()) + '\\nq: ' + self.q.__str__() + \\\n",
        "                '\\nv: ' + str(self.v.flatten())\n",
        "        s = s.replace('[ ', '[')\n",
        "        s = s.replace(', ', ' ')\n",
        "        s = s.replace(' ]', ']')\n",
        "        return s\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "    def __add__(self, other):\n",
        "        assert other.shape == (9, 1)\n",
        "        out = np.empty(self.arr.shape)\n",
        "        out[:3]  = self.p + other[:3]\n",
        "        out[3:7] = (self.q + other[3:6]).elements\n",
        "        out[7:]  = self.v + other[6:]\n",
        "        return State(out)\n",
        "    def __iadd__(self, other):\n",
        "        assert other.shape == (9, 1)\n",
        "        self.arr[:3] += other[:3]\n",
        "        self.arr[3:7] = (self.q + other[3:6]).elements\n",
        "        self.arr[7:] += other[6:]\n",
        "        return self\n",
        "    @property\n",
        "    def p(self):\n",
        "        return self.arr[:3]\n",
        "    @property\n",
        "    def q(self):\n",
        "        return Quat(self.arr[3:7])\n",
        "    @property\n",
        "    def v(self):\n",
        "        return self.arr[7:]\n",
        "    @property\n",
        "    def elements(self):\n",
        "        return self.arr.copy()\n",
        "    def copy(self):\n",
        "        return State(self.arr.copy())\n",
        "\n",
        "class Dynamics():\n",
        "    def __init__(self):\n",
        "        self.k1 = np.zeros((9,1))\n",
        "        self.k2 = np.zeros((9,1))\n",
        "        self.k3 = np.zeros((9,1))\n",
        "        self.k4 = np.zeros((9,1))\n",
        "        self.cd = 0.1\n",
        "        e_z = np.array([[0,0,1]]).T\n",
        "        self.g = 9.8065 * e_z\n",
        "        self.se = 0.5\n",
        "\n",
        "    def run(self, xu, dt):\n",
        "        x,u = State(xu[:10]), xu[10:]\n",
        "        self.k1 = self.f(x, u)\n",
        "        self.k2 = self.f(x + self.k1*(dt/2), u)\n",
        "        self.k3 = self.f(x + self.k2*(dt/2), u)\n",
        "        self.k4 = self.f(x + self.k3*dt, u)\n",
        "        # x += (self.k1 + 2*(self.k2 + self.k3) + self.k4) * (dt/6)\n",
        "        return x + (self.k1 + 2*(self.k2 + self.k3) + self.k4) * (dt/6)\n",
        "\n",
        "    def f(self, x, u):\n",
        "        s, w = u[0], u[1:]\n",
        "        dx = np.empty(self.k1.shape)\n",
        "        dx[:3] = x.q.rota(x.v)\n",
        "        dx[3:6] = w\n",
        "        dx[6:] = -self.g*(s/self.se) - self.cd*x.v + x.q.rotp(self.g) - \\\n",
        "                skew(w) @ x.v\n",
        "        return dx\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        return self.x.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oER-ve3bviM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator():\n",
        "  def __init__(self, num_states, num_inputs, dt=0.01, batch_size=50):\n",
        "    self.n = num_states\n",
        "    self.m = num_inputs\n",
        "    self.dt = dt\n",
        "    self.batch_size = batch_size\n",
        "    self.pos_lim = 50\n",
        "    self.att_lim = np.pi/3.5\n",
        "    self.vel_lim = 10\n",
        "    self.rate_lim = np.pi\n",
        "    self.s_lim = 1\n",
        "    self.dyn = Dynamics()\n",
        "\n",
        "  def getRandomInput(self):\n",
        "    xu = np.empty(self.n + self.m)\n",
        "    xu[:2] = np.random.uniform(-self.pos_lim, self.pos_lim, 2)\n",
        "    xu[2] = np.random.uniform(-self.pos_lim, 0)\n",
        "    mask = np.random.uniform(size=3) > 0.2\n",
        "    euler = np.random.uniform(-self.att_lim, self.att_lim, 3) * mask\n",
        "    xu[3:7] = Quat.from_euler(*euler).elements.flatten()\n",
        "    xu[7:10] = np.random.uniform(-self.vel_lim, self.vel_lim, 3)\n",
        "    xu[10] = np.random.uniform(0, self.s_lim)\n",
        "    xu[11:] = np.random.uniform(-self.rate_lim, self.rate_lim, 3)\n",
        "    return xu\n",
        "\n",
        "  def getBatch(self):\n",
        "    batch_in = np.empty((self.batch_size, self.n+self.m))\n",
        "    batch_out = np.empty((self.batch_size, self.n))\n",
        "    for i in range(self.batch_size):\n",
        "      batch_in[i] = self.getRandomInput()\n",
        "      batch_out[i] = self.dyn.run(batch_in[i].reshape(-1,1), self.dt).elements.flatten()\n",
        "    return batch_in, batch_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4SMzucDAkn5",
        "colab_type": "code",
        "outputId": "163ca357-80b2-4127-b441-c44ad1435a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "def testGen():\n",
        "  data_gen = DataGenerator(10,4,dt=0.01,batch_size=2)\n",
        "  ran = data_gen.getRandomInput()\n",
        "  print('rand: ', ran)\n",
        "\n",
        "  x, truth = data_gen.getBatch()\n",
        "  print('x: \\n', x)\n",
        "  print('truth: \\n', truth)\n",
        "\n",
        "  dyn = Dynamics()\n",
        "  for i, state in enumerate(x):\n",
        "    state = state.reshape(-1,1)\n",
        "    out = dyn.run(state, 0.01).elements.flatten()\n",
        "    error = out - truth[i]\n",
        "    norm = np.sqrt(error @ error)\n",
        "    print('norm: ', norm)\n",
        "\n",
        "testGen()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rand:  [ 2.75936258e+01 -1.73089776e+01 -3.54717030e+01  9.99712791e-01\n",
            "  0.00000000e+00 -2.39653168e-02  0.00000000e+00  1.22622049e+00\n",
            "  7.58786927e+00  7.92926085e+00  8.57177888e-01 -2.27884659e+00\n",
            " -3.05342169e+00 -1.88735337e+00]\n",
            "x: \n",
            " [[-1.03185866e+01 -3.87230079e+01 -4.65416270e+01  8.84354899e-01\n",
            "   1.18946817e-01 -3.88429472e-01 -2.29979591e-01 -1.62669139e+00\n",
            "  -7.15056905e+00 -7.36212226e+00  7.66513702e-01 -1.88145468e+00\n",
            "  -4.49874075e-01 -1.35745620e+00]\n",
            " [-4.33865105e+01 -1.43136318e+01 -6.40574177e+00  9.04050169e-01\n",
            "  -1.80586277e-02 -3.19024171e-01 -2.83885112e-01  8.70532583e+00\n",
            "   9.12577907e+00  9.30118840e+00  8.36135920e-01 -1.01610418e+00\n",
            "   1.85659858e+00 -7.41482520e-01]]\n",
            "truth: \n",
            " [[-1.02955505e+01 -3.87744260e+01 -4.66290247e+01  8.82977500e-01\n",
            "   1.12738359e-01 -3.87420813e-01 -2.39887288e-01 -1.50085163e+00\n",
            "  -6.98634232e+00 -7.56504179e+00]\n",
            " [-4.33355788e+01 -1.42611402e+01 -6.26730090e+00  9.05810570e-01\n",
            "  -1.88324643e-02 -3.09236695e-01 -2.89007292e-01  8.51178234e+00\n",
            "   9.09955997e+00  9.45888910e+00]]\n",
            "norm:  0.0\n",
            "norm:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XvwSOVc3-Sy",
        "colab_type": "code",
        "outputId": "d2734893-b285-48f4-f82d-c25c68a4c1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def testLinearSize():\n",
        "  inputs = 14\n",
        "  outputs = 10\n",
        "  batch = 1\n",
        "  x_test = torch.zeros(batch,inputs)\n",
        "  up = nn.Linear(inputs, 50)\n",
        "  up_test = up(x_test)\n",
        "  print('up: ', up_test.size())\n",
        "\n",
        "  down = nn.Linear(50, outputs)\n",
        "  down_test = down(up_test)\n",
        "  print('down: ', down_test.size())\n",
        "\n",
        "testLinearSize()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "up:  torch.Size([1, 50])\n",
            "down:  torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9EXL6hvwr08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, skip=False, end_activation=False):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.skip = skip\n",
        "    self.end_act = end_activation\n",
        "    self.activation = nn.ReLU()\n",
        "    self.layer1 = nn.Linear(dim_in, dim_in)\n",
        "    self.layer2 = nn.Linear(dim_in, dim_in)\n",
        "    self.layer3 = nn.Linear(dim_in, dim_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.activation(self.layer1(x))\n",
        "    out2 = self.activation(self.layer2(out1))\n",
        "    skip = out2 + out1\n",
        "    if self.end_act:\n",
        "      out3 = self.activation(self.layer3(skip))\n",
        "    else:\n",
        "      out3 = self.layer3(skip)\n",
        "    if self.skip:\n",
        "      return skip, out3\n",
        "    else:\n",
        "      return out3\n",
        "\n",
        "class DynamicsNN(nn.Module):\n",
        "  def __init__(self, num_states, num_inputs, dt):\n",
        "    super(DynamicsNN, self).__init__()\n",
        "    self.dt = dt\n",
        "    self.activation = nn.ReLU()\n",
        "    self.up1 = nn.Linear(num_states+num_inputs, 100)\n",
        "    self.up2 = ResBlock(100,  200, skip=True)\n",
        "    self.up3 = ResBlock(200, 400, skip=True)\n",
        "    self.dn1 = ResBlock(400, 200)\n",
        "    self.dn2 = ResBlock(200, 100)\n",
        "    self.dn3 = ResBlock(100,  num_states, end_activation=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    up1 = self.activation(self.up1(x))\n",
        "    skip1, up2 = self.up2(up1)\n",
        "    skip2, up3 = self.up3(up2)\n",
        "    down1 = self.dn1(up3)\n",
        "    down2 = self.dn2(skip2 + down1)\n",
        "    down3 = self.dn3(skip1 + down2)\n",
        "    p = down3[:,:3] + x[:,:3] + x[:,7:10]*self.dt\n",
        "    w = torch.zeros_like(down3[:,3:7])\n",
        "    w[:,1:] = x[:,11:]\n",
        "    q = down3[:,3:7] + x[:,3:7] + w*self.dt\n",
        "    q1 = torch.zeros_like(q)\n",
        "    q1 = q / torch.norm(q, dim=1, keepdim=True)\n",
        "    # was getting nans here, but removing ReLU on output layer fixed it\n",
        "    # norm = torch.norm(q, dim=1)\n",
        "    # mask = norm != 0.0\n",
        "    # q1[mask] = q[mask] / norm[mask].unsqueeze(-1)\n",
        "    v = down3[:,7:] + x[:,7:10]\n",
        "    out = torch.cat((p,q1,v), dim=1)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV0Mc_4YUWlt",
        "colab_type": "code",
        "outputId": "c2f988dc-b3f7-49a6-8620-c37533eceebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "def testNet():\n",
        "  n = 10\n",
        "  m = 4\n",
        "  dt = 0.01\n",
        "  x_test = torch.randn(2,n+m)\n",
        "  print('x_test: ', x_test)\n",
        "  net = DynamicsNN(n,m, dt)\n",
        "  test = net(x_test)\n",
        "  print('shape: ', test.shape)\n",
        "  print('output: ', test)\n",
        "\n",
        "testNet()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test:  tensor([[ 0.0384,  0.0245, -0.3869, -1.0986, -1.3262, -0.5020,  1.4298, -2.3050,\n",
            "         -1.9592,  1.3372, -0.2858,  0.2833,  0.8652,  0.8989],\n",
            "        [ 1.6104,  0.2954, -1.5759, -0.3991, -1.5973, -0.8956,  1.8757, -0.1768,\n",
            "          0.3394,  0.9204,  0.1510, -0.3747,  0.3136,  0.4136]])\n",
            "shape:  torch.Size([2, 10])\n",
            "output:  tensor([[ 0.0154,  0.0639, -0.3546, -0.4711, -0.5239, -0.2302,  0.6712, -2.2166,\n",
            "         -1.9592,  1.3372],\n",
            "        [ 1.6087,  0.3391, -1.5345, -0.1082, -0.5512, -0.3503,  0.7496, -0.0105,\n",
            "          0.3394,  0.9204]], grad_fn=<CatBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSv0Pa_ezzXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcLoss(loss_fn, truth, pred):\n",
        "  position_loss = loss_fn(pred[:,:3], truth[:,:3])\n",
        "  attitude_loss = loss_fn(pred[:,3:7], truth[:,3:7])*100\n",
        "  velocity_loss = loss_fn(pred[:,7:], truth[:,7:])\n",
        "  loss = position_loss + attitude_loss + velocity_loss\n",
        "  return loss\n",
        "\n",
        "def validate(model, dt, optimizer, loss_fn):\n",
        "  with torch.no_grad():\n",
        "    dyn = Dynamics()\n",
        "    x0 = np.array([0.,0,-5,1,0,0,0,0,0,0])\n",
        "    low = np.zeros(4)\n",
        "    low[1:] = -np.pi\n",
        "    high = np.ones(4)\n",
        "    high[1:] = np.pi\n",
        "    u_vals = np.array([[0.7, 0, 0, 0],  # go up\n",
        "                       [0.4, 0, 0, 0],  # go down\n",
        "                       [0.5, 1, 0, 0],  # roll right\n",
        "                       [0.5, -1, 0, 0], # roll left\n",
        "                       [0.5, 0, 1, 0],  # pitch backward\n",
        "                       [0.5, 0, -1, 0], # pitch foreward\n",
        "                       [0.5, 0, 0, 1],  # yaw right\n",
        "                       [0.5, 0, 0, -1], # yaw left\n",
        "                       [0.6, 1, 1, 0],  # pitch and roll\n",
        "                       [.6, -1, -1, 0], # pitch and roll\n",
        "                       [0.6, 1, -1, 0], # pitch and roll\n",
        "                       [0.6, -1, 1, 0], # pitch and roll\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       np.random.uniform(low=low, high=high, size=4),\n",
        "                       ])\n",
        "    \n",
        "    v_k = np.block([np.tile(x0, (len(u_vals),1)), u_vals])\n",
        "    truth = np.zeros((len(u_vals), 10))\n",
        "    for i in range(len(u_vals)):\n",
        "      truth[i] = dyn.run(v_k[i].reshape(-1,1), dt).elements.flatten()\n",
        "    \n",
        "    truth = torch.from_numpy(truth).float().cuda()\n",
        "    v_k = torch.from_numpy(v_k).float().cuda()\n",
        "    \n",
        "    pred = model(v_k)\n",
        "\n",
        "    loss = calcLoss(loss_fn, truth, pred).item()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_yf9ICTpBo",
        "colab_type": "code",
        "outputId": "6d361903-879f-449f-946b-ea3783302ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "def train(save_model):\n",
        "  n = 10\n",
        "  m = 4\n",
        "  dt = 0.01\n",
        "  model = DynamicsNN(n, m, dt).cuda()\n",
        "\n",
        "  lr = 1e-5\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "  loss_fn = nn.MSELoss()\n",
        "\n",
        "  epochs = 1000\n",
        "  batch_size = 500\n",
        "  data_gen = DataGenerator(n, m, dt, batch_size)\n",
        "\n",
        "  save_every = 10\n",
        "  decay_every = 100\n",
        "  validate_every = 50\n",
        "\n",
        "  epoch_hist = []\n",
        "  loss_hist = []\n",
        "  v_loss_hist = []\n",
        "  v_epoch_hist = []\n",
        "\n",
        "  num_nans = 0\n",
        "\n",
        "  last_l = 0\n",
        "  last_e = 0\n",
        "  info = tqdm(total=epochs, position=0, leave=False)\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    xu_t, x_tp1 = data_gen.getBatch()\n",
        "    in_var = Variable(torch.from_numpy(xu_t).float(), requires_grad=True).cuda()\n",
        "    out_truth = Variable(torch.from_numpy(x_tp1).float(), requires_grad=False).cuda()\n",
        "\n",
        "    out_pred = model(in_var)\n",
        "\n",
        "    loss = calcLoss(loss_fn, out_truth, out_pred)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # was getting nans in out_pred and loss because / by 0 in norm calculation \n",
        "    # until I removed ReLU from last layer\n",
        "    loss = loss.item()\n",
        "    if np.isnan(loss):\n",
        "      num_nans += 1\n",
        "\n",
        "    if epoch % validate_every == 0:\n",
        "      v_loss_hist.append(validate(model, dt, optimizer, loss_fn))\n",
        "      v_epoch_hist.append(epoch)\n",
        "\n",
        "    if epoch % save_every == 0:\n",
        "      last_l = loss\n",
        "      epoch_hist.append(epoch)\n",
        "      loss_hist.append(loss)\n",
        "      if save_model:\n",
        "        torch.save(model, 'learned_quadrotor_model.pt')\n",
        "    \n",
        "    if epoch % decay_every == 0:\n",
        "      lr *= 0.9\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    info.set_description('Last saved loss: {:.3f}'.format(last_l))\n",
        "    info.update(1)\n",
        "  \n",
        "  fig, ax = plt.subplots()\n",
        "  train_losses, = ax.plot(epoch_hist, loss_hist, 'b', label='train')\n",
        "  val_losses, = ax.plot(v_epoch_hist, v_loss_hist, 'r', label='validate')\n",
        "  ax.set_xlabel('Epoch')\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.legend()\n",
        "  print('\\nNans: ', num_nans)\n",
        "  print('sample input: ', xu_t[0])\n",
        "  print('true output: ', x_tp1[0])\n",
        "  print('predicted output: ', out_pred[0])\n",
        "  plt.show()\n",
        "\n",
        "train(save_model=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Nans:  0\n",
            "sample input:  [ 12.81297009 -47.87616067 -36.23742365   0.97612334   0.\n",
            "   0.           0.21721698  -8.30838432  -1.37638559  -8.77966178\n",
            "   0.37354556   1.72504372   0.49738194  -2.98550099]\n",
            "true output:  [ 1.27435966e+01 -4.79238328e+01 -3.63250524e+01  9.79217609e-01\n",
            "  7.87867978e-03  4.30085706e-03  2.02613682e-01 -8.20974514e+00\n",
            " -1.77186379e+00 -8.76001674e+00]\n",
            "predicted output:  tensor([ 1.2960e+01, -4.7890e+01, -3.6325e+01,  9.8191e-01,  1.7353e-02,\n",
            "         5.0033e-03,  1.8847e-01, -8.3084e+00, -1.3764e+00, -8.7797e+00],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGRlBQQIiKSkgJCRERSUkCIiEhK/w1r3UYGU6wQ8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}